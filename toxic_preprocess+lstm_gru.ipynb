{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "APPO = {\n",
    "\"aren't\" : \"are not\",\n",
    "\"can't\" : \"cannot\",\n",
    "\"couldn't\" : \"could not\",\n",
    "\"didn't\" : \"did not\",\n",
    "\"doesn't\" : \"does not\",\n",
    "\"don't\" : \"do not\",\n",
    "\"hadn't\" : \"had not\",\n",
    "\"hasn't\" : \"has not\",\n",
    "\"haven't\" : \"have not\",\n",
    "\"he'd\" : \"he would\",\n",
    "\"he'll\" : \"he will\",\n",
    "\"he's\" : \"he is\",\n",
    "\"i'd\" : \"I would\",\n",
    "\"i'd\" : \"I had\",\n",
    "\"i'll\" : \"I will\",\n",
    "\"i'm\" : \"I am\",\n",
    "\"isn't\" : \"is not\",\n",
    "\"it's\" : \"it is\",\n",
    "\"it'll\":\"it will\",\n",
    "\"i've\" : \"I have\",\n",
    "\"let's\" : \"let us\",\n",
    "\"mightn't\" : \"might not\",\n",
    "\"mustn't\" : \"must not\",\n",
    "\"shan't\" : \"shall not\",\n",
    "\"she'd\" : \"she would\",\n",
    "\"she'll\" : \"she will\",\n",
    "\"she's\" : \"she is\",\n",
    "\"shouldn't\" : \"should not\",\n",
    "\"that's\" : \"that is\",\n",
    "\"there's\" : \"there is\",\n",
    "\"they'd\" : \"they would\",\n",
    "\"they'll\" : \"they will\",\n",
    "\"they're\" : \"they are\",\n",
    "\"they've\" : \"they have\",\n",
    "\"we'd\" : \"we would\",\n",
    "\"we're\" : \"we are\",\n",
    "\"weren't\" : \"were not\",\n",
    "\"we've\" : \"we have\",\n",
    "\"what'll\" : \"what will\",\n",
    "\"what're\" : \"what are\",\n",
    "\"what's\" : \"what is\",\n",
    "\"what've\" : \"what have\",\n",
    "\"where's\" : \"where is\",\n",
    "\"who'd\" : \"who would\",\n",
    "\"who'll\" : \"who will\",\n",
    "\"who're\" : \"who are\",\n",
    "\"who's\" : \"who is\",\n",
    "\"who've\" : \"who have\",\n",
    "\"won't\" : \"will not\",\n",
    "\"wouldn't\" : \"would not\",\n",
    "\"you'd\" : \"you would\",\n",
    "\"you'll\" : \"you will\",\n",
    "\"you're\" : \"you are\",\n",
    "\"you've\" : \"you have\",\n",
    "\"'re\": \" are\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'll\":\" will\",\n",
    "\"didn't\": \"did not\",\n",
    "\"tryin'\":\"trying\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import TweetTokenizer  \n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "eng_stopwords = set(stopwords.words(\"english\"))\n",
    "lem = WordNetLemmatizer()\n",
    "tweettokenizer=TweetTokenizer()\n",
    "\n",
    "def clean(comment):\n",
    "    #remove new line\n",
    "    comment=re.sub(\"\\\\n\",\" \",comment)\n",
    "    # remove leaky elements like ip,user\n",
    "    comment=re.sub(\"\\d{1,3}.\\d{1,3}.\\d{1,3}.\\d{1,3}\",\"\",comment)\n",
    "    #removing usernames\n",
    "    comment=re.sub(\"\\[\\[.*\\]\",\"\",comment)\n",
    "    #removing multiple space\n",
    "    comment=' '.join(comment.split())\n",
    "    \n",
    "    \n",
    "    words=tweettokenizer.tokenize(comment)\n",
    "    \n",
    "    # (')aphostophe  replacement (ie)   you're --> you are  \n",
    "    # ( basic dictionary lookup : master dictionary present in a hidden block of code)\n",
    "    words=[APPO[word] if word in APPO else word for word in words]\n",
    "    words=[lem.lemmatize(word, \"v\") for word in words]\n",
    "    # words = [w for w in words if not w in eng_stopwords]\n",
    "    \n",
    "    clean_sent=\" \".join(words)\n",
    "    # remove any non alphanum,digit character\n",
    "    #clean_sent=re.sub(\"\\W+\",\" \",clean_sent)\n",
    "    #clean_sent=re.sub(\"  \",\" \",clean_sent)\n",
    "    return(clean_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_file='./dataset/glove.6B.50d.txt'\n",
    "train_file='./dataset/train.csv'\n",
    "test_file='./dataset/test.csv'\n",
    "\n",
    "train=pd.read_csv(train_file)\n",
    "test=pd.read_csv(test_file)\n",
    "\n",
    "sent_train=train[\"comment_text\"].fillna(\"nan\")\n",
    "\n",
    "classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y=train[classes].values\n",
    "\n",
    "sent_test=test[\"comment_text\"].fillna(\"nan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_train[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean(sent_train[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_sent_train=sent_train.apply(lambda x:clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'explanation edit make username hardcore metallica fan revert werent vandalisms closure gas vote new york dolls fac please dont remove template talk page since im retire'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_sent_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_sent_test=sent_test.apply(lambda x:clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train=pd.read_csv(train_file)\n",
    "clean_test=pd.read_csv(test_file)\n",
    "\n",
    "clean_train[\"comment_text\"]=clean_sent_train\n",
    "clean_test[\"comment_text\"]=clean_sent_test\n",
    "\n",
    "clean_train.to_csv('clean_train.csv', index=False)\n",
    "clean_test.to_csv('clean_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words_count=20000\n",
    "embedding_size=50\n",
    "max_words_length=100\n",
    "\n",
    "tokenizer=Tokenizer(num_words=max_words_count)\n",
    "tokenizer.fit_on_texts(clean_sent_train)\n",
    "tokens_train = tokenizer.texts_to_sequences(clean_sent_train)\n",
    "tokens_test = tokenizer.texts_to_sequences(clean_sent_test)\n",
    "\n",
    "x_train=pad_sequences(tokens_train,maxlen=max_words_length)\n",
    "x_test=pad_sequences(tokens_test,maxlen=max_words_length)\n",
    "\n",
    "def index_to_embed(word,*embedding):\n",
    "    return word,np.asarray(embedding,dtype='float32')\n",
    "\n",
    "embed_dict=dict(index_to_embed(*o.strip().split())for o in open(glove_file))\n",
    "\n",
    "all_embs = np.stack(embed_dict.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "\n",
    "word_idx=tokenizer.word_index\n",
    "\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (max_words_count, embedding_size))\n",
    "\n",
    "for word,i in word_idx.items():\n",
    "    if i < max_words_count:\n",
    "        vec_temp=embed_dict.get(word)\n",
    "        if vec_temp is not None:\n",
    "            embedding_matrix[i]=vec_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143584/143613 [============================>.] - ETA: 0s - loss: 0.0599 - acc: 0.9793Epoch 00000: val_loss improved from inf to 0.04932, saving model to lstm_preprocess_best.hdf5\n",
      "143613/143613 [==============================] - 1559s - loss: 0.0599 - acc: 0.9793 - val_loss: 0.0493 - val_acc: 0.9816\n",
      "Epoch 2/2\n",
      "143584/143613 [============================>.] - ETA: 0s - loss: 0.0453 - acc: 0.9828Epoch 00001: val_loss improved from 0.04932 to 0.04715, saving model to lstm_preprocess_best.hdf5\n",
      "143613/143613 [==============================] - 1544s - loss: 0.0453 - acc: 0.9828 - val_loss: 0.0472 - val_acc: 0.9822\n",
      "153164/153164 [==============================] - 244s   \n"
     ]
    }
   ],
   "source": [
    "inp=Input(shape=(max_words_length,))\n",
    "x=Embedding(max_words_count,embedding_size,weights=[embedding_matrix])(inp)\n",
    "x=Bidirectional(LSTM(embedding_size,return_sequences=True,dropout=0.1,recurrent_dropout=0.1))(x)\n",
    "x=GlobalMaxPool1D()(x)\n",
    "x = Dense(50, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(6, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "file_path=\"lstm_preprocess_best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=20)\n",
    "callbacks_list = [checkpoint, early] #early\n",
    "model.fit(x_train, y, batch_size=32, epochs=2, validation_split=0.1, callbacks=callbacks_list)\n",
    "\n",
    "y_test = model.predict([x_test], batch_size=1024, verbose=1)\n",
    "sample_submission = pd.read_csv('./dataset/sample_submission.csv')\n",
    "sample_submission[classes] = y_test\n",
    "sample_submission.to_csv('submission/submission_preprocess_lstm.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143584/143613 [============================>.] - ETA: 0s - loss: 0.0588 - acc: 0.9794Epoch 00000: val_loss improved from inf to 0.04906, saving model to gru_preprocess_best.hdf5\n",
      "143613/143613 [==============================] - 1324s - loss: 0.0588 - acc: 0.9794 - val_loss: 0.0491 - val_acc: 0.9816\n",
      "Epoch 2/2\n",
      "143584/143613 [============================>.] - ETA: 0s - loss: 0.0452 - acc: 0.9828Epoch 00001: val_loss improved from 0.04906 to 0.04665, saving model to gru_preprocess_best.hdf5\n",
      "143613/143613 [==============================] - 1317s - loss: 0.0452 - acc: 0.9828 - val_loss: 0.0466 - val_acc: 0.9824\n",
      "153164/153164 [==============================] - 212s   \n"
     ]
    }
   ],
   "source": [
    "from keras.layers import GRU\n",
    "inp_1=Input(shape=(max_words_length,))\n",
    "x_1=Embedding(max_words_count,embedding_size,weights=[embedding_matrix])(inp_1)\n",
    "x_1=Bidirectional(GRU(embedding_size,return_sequences=True,dropout=0.1,recurrent_dropout=0.1))(x_1)\n",
    "x_1=GlobalMaxPool1D()(x_1)\n",
    "x_1 = Dense(50, activation=\"relu\")(x_1)\n",
    "x_1 = Dropout(0.1)(x_1)\n",
    "x_1 = Dense(6, activation=\"sigmoid\")(x_1)\n",
    "model1 = Model(inputs=inp_1, outputs=x_1)\n",
    "model1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "file_path=\"gru_preprocess_best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=20)\n",
    "callbacks_list = [checkpoint, early] #early\n",
    "model1.fit(x_train, y, batch_size=32, epochs=2, validation_split=0.1, callbacks=callbacks_list)\n",
    "\n",
    "y_test_1 = model1.predict([x_test], batch_size=1024, verbose=1)\n",
    "sample_submission = pd.read_csv('./dataset/sample_submission.csv')\n",
    "sample_submission[classes] = y_test_1\n",
    "sample_submission.to_csv('submission/submission_preprocess_GRU.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_lstm='submission/submission_preprocess_lstm.csv'\n",
    "file_GRU='submission/submission_preprocess_GRU.csv'\n",
    "p_lstm = pd.read_csv(file_lstm)\n",
    "p_gru = pd.read_csv(file_GRU)\n",
    "\n",
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "p_res_avg = p_lstm.copy()\n",
    "p_res_avg[label_cols] = (p_gru[label_cols] + p_lstm[label_cols]) / 2\n",
    "\n",
    "p_res_avg.to_csv('submission_preprocess_lstm+gru_avg.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
