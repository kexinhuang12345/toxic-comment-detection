{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "APPO = {\n",
    "\"aren't\" : \"are not\",\n",
    "\"can't\" : \"cannot\",\n",
    "\"couldn't\" : \"could not\",\n",
    "\"didn't\" : \"did not\",\n",
    "\"doesn't\" : \"does not\",\n",
    "\"don't\" : \"do not\",\n",
    "\"hadn't\" : \"had not\",\n",
    "\"hasn't\" : \"has not\",\n",
    "\"haven't\" : \"have not\",\n",
    "\"he'd\" : \"he would\",\n",
    "\"he'll\" : \"he will\",\n",
    "\"he's\" : \"he is\",\n",
    "\"i'd\" : \"I would\",\n",
    "\"i'd\" : \"I had\",\n",
    "\"i'll\" : \"I will\",\n",
    "\"i'm\" : \"I am\",\n",
    "\"isn't\" : \"is not\",\n",
    "\"it's\" : \"it is\",\n",
    "\"it'll\":\"it will\",\n",
    "\"i've\" : \"I have\",\n",
    "\"let's\" : \"let us\",\n",
    "\"mightn't\" : \"might not\",\n",
    "\"mustn't\" : \"must not\",\n",
    "\"shan't\" : \"shall not\",\n",
    "\"she'd\" : \"she would\",\n",
    "\"she'll\" : \"she will\",\n",
    "\"she's\" : \"she is\",\n",
    "\"shouldn't\" : \"should not\",\n",
    "\"that's\" : \"that is\",\n",
    "\"there's\" : \"there is\",\n",
    "\"they'd\" : \"they would\",\n",
    "\"they'll\" : \"they will\",\n",
    "\"they're\" : \"they are\",\n",
    "\"they've\" : \"they have\",\n",
    "\"we'd\" : \"we would\",\n",
    "\"we're\" : \"we are\",\n",
    "\"weren't\" : \"were not\",\n",
    "\"we've\" : \"we have\",\n",
    "\"what'll\" : \"what will\",\n",
    "\"what're\" : \"what are\",\n",
    "\"what's\" : \"what is\",\n",
    "\"what've\" : \"what have\",\n",
    "\"where's\" : \"where is\",\n",
    "\"who'd\" : \"who would\",\n",
    "\"who'll\" : \"who will\",\n",
    "\"who're\" : \"who are\",\n",
    "\"who's\" : \"who is\",\n",
    "\"who've\" : \"who have\",\n",
    "\"won't\" : \"will not\",\n",
    "\"wouldn't\" : \"would not\",\n",
    "\"you'd\" : \"you would\",\n",
    "\"you'll\" : \"you will\",\n",
    "\"you're\" : \"you are\",\n",
    "\"you've\" : \"you have\",\n",
    "\"'re\": \" are\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'll\":\" will\",\n",
    "\"didn't\": \"did not\",\n",
    "\"tryin'\":\"trying\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import TweetTokenizer  \n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "eng_stopwords = set(stopwords.words(\"english\"))\n",
    "lem = WordNetLemmatizer()\n",
    "tweettokenizer=TweetTokenizer()\n",
    "\n",
    "def clean(comment):\n",
    "    #remove new line\n",
    "    comment=re.sub(\"\\\\n\",\" \",comment)\n",
    "    # remove leaky elements like ip,user\n",
    "    comment=re.sub(\"\\d{1,3}.\\d{1,3}.\\d{1,3}.\\d{1,3}\",\"\",comment)\n",
    "    #removing usernames\n",
    "    comment=re.sub(\"\\[\\[.*\\]\",\"\",comment)\n",
    "    #removing multiple space\n",
    "    comment=' '.join(comment.split())\n",
    "    \n",
    "    \n",
    "    words=tweettokenizer.tokenize(comment)\n",
    "    \n",
    "    # (')aphostophe  replacement (ie)   you're --> you are  \n",
    "    # ( basic dictionary lookup : master dictionary present in a hidden block of code)\n",
    "    words=[APPO[word] if word in APPO else word for word in words]\n",
    "    words=[lem.lemmatize(word, \"v\") for word in words]\n",
    "    # words = [w for w in words if not w in eng_stopwords]\n",
    "    \n",
    "    clean_sent=\" \".join(words)\n",
    "    # remove any non alphanum,digit character\n",
    "    #clean_sent=re.sub(\"\\W+\",\" \",clean_sent)\n",
    "    #clean_sent=re.sub(\"  \",\" \",clean_sent)\n",
    "    return(clean_sent).encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_file='./dataset/glove.twitter.27B.50d.txt'\n",
    "train_file='./dataset/train.csv'\n",
    "test_file='./dataset/test.csv'\n",
    "\n",
    "train=pd.read_csv(train_file)\n",
    "test=pd.read_csv(test_file)\n",
    "\n",
    "sent_train=train[\"comment_text\"].fillna(\"nan\")\n",
    "\n",
    "classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y=train[classes].values\n",
    "\n",
    "sent_test=test[\"comment_text\"].fillna(\"nan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words_count=20000\n",
    "embedding_size=50\n",
    "max_words_length=100\n",
    "\n",
    "tokenizer=Tokenizer(num_words=max_words_count)\n",
    "tokenizer.fit_on_texts(clean_sent_train)\n",
    "tokens_train = tokenizer.texts_to_sequences(clean_sent_train)\n",
    "tokens_test = tokenizer.texts_to_sequences(clean_sent_test)\n",
    "\n",
    "x_train=pad_sequences(tokens_train,maxlen=max_words_length)\n",
    "x_test=pad_sequences(tokens_test,maxlen=max_words_length)\n",
    "\n",
    "def index_to_embed(word,*embedding):\n",
    "    return word,np.asarray(embedding,dtype='float32')\n",
    "\n",
    "embed_dict=dict(index_to_embed(*o.strip().split())for o in open(glove_file))\n",
    "\n",
    "all_embs = np.stack(embed_dict.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "\n",
    "word_idx=tokenizer.word_index\n",
    "\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (max_words_count, embedding_size))\n",
    "\n",
    "for word,i in word_idx.items():\n",
    "    if i < max_words_count:\n",
    "        vec_temp=embed_dict.get(word)\n",
    "        if vec_temp is not None:\n",
    "            embedding_matrix[i]=vec_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143584/143613 [============================>.] - ETA: 0s - loss: 0.0595 - acc: 0.9794Epoch 00000: val_loss improved from inf to 0.04869, saving model to lstm_preprocess2_best.hdf5\n",
      "143613/143613 [==============================] - 1717s - loss: 0.0595 - acc: 0.9794 - val_loss: 0.0487 - val_acc: 0.9822\n",
      "Epoch 2/2\n",
      "143584/143613 [============================>.] - ETA: 0s - loss: 0.0446 - acc: 0.9829Epoch 00001: val_loss improved from 0.04869 to 0.04683, saving model to lstm_preprocess2_best.hdf5\n",
      "143613/143613 [==============================] - 1720s - loss: 0.0446 - acc: 0.9829 - val_loss: 0.0468 - val_acc: 0.9823\n",
      "153164/153164 [==============================] - 242s   \n"
     ]
    }
   ],
   "source": [
    "inp=Input(shape=(max_words_length,))\n",
    "x=Embedding(max_words_count,embedding_size,weights=[embedding_matrix])(inp)\n",
    "x=Bidirectional(LSTM(embedding_size,return_sequences=True,dropout=0.1,recurrent_dropout=0.1))(x)\n",
    "x=GlobalMaxPool1D()(x)\n",
    "x = Dense(50, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(6, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "file_path=\"lstm_preprocess2_best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=20)\n",
    "callbacks_list = [checkpoint, early] #early\n",
    "model.fit(x_train, y, batch_size=32, epochs=2, validation_split=0.1, callbacks=callbacks_list)\n",
    "\n",
    "y_test = model.predict([x_test], batch_size=1024, verbose=1)\n",
    "sample_submission = pd.read_csv('./dataset/sample_submission.csv')\n",
    "sample_submission[classes] = y_test\n",
    "sample_submission.to_csv('submission/submission_preprocess2_lstm.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143584/143613 [============================>.] - ETA: 0s - loss: 0.0595 - acc: 0.9794Epoch 00000: val_loss improved from inf to 0.04869, saving model to lstm_preprocess2_best.hdf5\n",
      "143613/143613 [==============================] - 1717s - loss: 0.0595 - acc: 0.9794 - val_loss: 0.0487 - val_acc: 0.9822\n",
      "Epoch 2/2\n",
      "143584/143613 [============================>.] - ETA: 0s - loss: 0.0446 - acc: 0.9829Epoch 00001: val_loss improved from 0.04869 to 0.04683, saving model to lstm_preprocess2_best.hdf5\n",
      "143613/143613 [==============================] - 1720s - loss: 0.0446 - acc: 0.9829 - val_loss: 0.0468 - val_acc: 0.9823\n",
      "153164/153164 [==============================] - 242s   \n"
     ]
    }
   ],
   "source": [
    "inp=Input(shape=(max_words_length,))\n",
    "x=Embedding(max_words_count,embedding_size,weights=[embedding_matrix])(inp)\n",
    "x=Bidirectional(LSTM(embedding_size,return_sequences=True,dropout=0.1,recurrent_dropout=0.1))(x)\n",
    "x=GlobalMaxPool1D()(x)\n",
    "x = Dense(50, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(6, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "file_path=\"lstm_preprocess2_best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=20)\n",
    "callbacks_list = [checkpoint, early] #early\n",
    "model.fit(x_train, y, batch_size=32, epochs=2, validation_split=0.1, callbacks=callbacks_list)\n",
    "\n",
    "y_test = model.predict([x_test], batch_size=1024, verbose=1)\n",
    "sample_submission = pd.read_csv('./dataset/sample_submission.csv')\n",
    "sample_submission[classes] = y_test\n",
    "sample_submission.to_csv('submission/submission_preprocess2_lstm.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143584/143613 [============================>.] - ETA: 0s - loss: 0.0578 - acc: 0.9797Epoch 00000: val_loss improved from inf to 0.04744, saving model to gru_glovetwitter_best.hdf5\n",
      "143613/143613 [==============================] - 1365s - loss: 0.0578 - acc: 0.9797 - val_loss: 0.0474 - val_acc: 0.9827\n",
      "Epoch 2/2\n",
      "143584/143613 [============================>.] - ETA: 0s - loss: 0.0440 - acc: 0.9832Epoch 00001: val_loss improved from 0.04744 to 0.04588, saving model to gru_glovetwitter_best.hdf5\n",
      "143613/143613 [==============================] - 1363s - loss: 0.0440 - acc: 0.9832 - val_loss: 0.0459 - val_acc: 0.9825\n",
      "153164/153164 [==============================] - 188s   \n"
     ]
    }
   ],
   "source": [
    "from keras.layers import GRU\n",
    "inp_1=Input(shape=(max_words_length,))\n",
    "x_1=Embedding(max_words_count,embedding_size,weights=[embedding_matrix])(inp_1)\n",
    "x_1=Bidirectional(GRU(embedding_size,return_sequences=True,dropout=0.1,recurrent_dropout=0.1))(x_1)\n",
    "x_1=GlobalMaxPool1D()(x_1)\n",
    "x_1 = Dense(50, activation=\"relu\")(x_1)\n",
    "x_1 = Dropout(0.1)(x_1)\n",
    "x_1 = Dense(6, activation=\"sigmoid\")(x_1)\n",
    "model1 = Model(inputs=inp_1, outputs=x_1)\n",
    "model1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "file_path=\"gru_glovetwitter_best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=20)\n",
    "callbacks_list = [checkpoint, early] #early\n",
    "model1.fit(x_train, y, batch_size=32, epochs=2, validation_split=0.1, callbacks=callbacks_list)\n",
    "\n",
    "y_test_1 = model1.predict([x_test], batch_size=1024, verbose=1)\n",
    "sample_submission = pd.read_csv('./dataset/sample_submission.csv')\n",
    "sample_submission[classes] = y_test_1\n",
    "sample_submission.to_csv('submission/submission_glovetwitter_GRU_preprocess3_leaky.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_lstm='submission/submission_preprocess_lstm.csv'\n",
    "file_GRU='submission/submission_preprocess_GRU.csv'\n",
    "p_lstm = pd.read_csv(file_lstm)\n",
    "p_gru = pd.read_csv(file_GRU)\n",
    "\n",
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "p_res_avg = p_lstm.copy()\n",
    "p_res_avg[label_cols] = (p_gru[label_cols] + p_lstm[label_cols]) / 2\n",
    "\n",
    "p_res_avg.to_csv('submission_preprocess2_lstm+gru_avg.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_best='submission/submission_glovetwitter_GRU_0.0457.csv'\n",
    "data=pd.read_csv(file_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
