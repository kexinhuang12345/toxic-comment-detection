{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_file='./dataset/glove.twitter.27B.50d.txt'\n",
    "train_file='./dataset/clean_train_v2.csv'\n",
    "test_file='./dataset/clean_test_v2.csv'\n",
    "\n",
    "train=pd.read_csv(train_file)\n",
    "test=pd.read_csv(test_file)\n",
    "\n",
    "sent_train=train[\"comment_text\"].fillna(\"nan\")\n",
    "\n",
    "classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y=train[classes].values\n",
    "\n",
    "sent_test=test[\"comment_text\"].fillna(\"nan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words_count=30000\n",
    "embedding_size=50\n",
    "max_words_length=128\n",
    "\n",
    "tokenizer=Tokenizer(num_words=max_words_count)\n",
    "tokenizer.fit_on_texts(sent_train)\n",
    "tokens_train = tokenizer.texts_to_sequences(sent_train)\n",
    "tokens_test = tokenizer.texts_to_sequences(sent_test)\n",
    "\n",
    "x_train=pad_sequences(tokens_train,maxlen=max_words_length)\n",
    "x_test=pad_sequences(tokens_test,maxlen=max_words_length)\n",
    "\n",
    "def index_to_embed(word,*embedding):\n",
    "    return word,np.asarray(embedding,dtype='float32')\n",
    "\n",
    "embed_dict=dict(index_to_embed(*o.strip().split())for o in open(glove_file))\n",
    "\n",
    "all_embs = np.stack(embed_dict.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "\n",
    "word_idx=tokenizer.word_index\n",
    "\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (max_words_count, embedding_size))\n",
    "\n",
    "for word,i in word_idx.items():\n",
    "    if i < max_words_count:\n",
    "        vec_temp=embed_dict.get(word)\n",
    "        if vec_temp is not None:\n",
    "            embedding_matrix[i]=vec_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143584/143613 [============================>.] - ETA: 0s - loss: 0.0561 - acc: 0.9802Epoch 00000: val_loss improved from inf to 0.04607, saving model to lstm_baseline.hdf5\n",
      "143613/143613 [==============================] - 2098s - loss: 0.0561 - acc: 0.9802 - val_loss: 0.0461 - val_acc: 0.9825\n",
      "Epoch 2/2\n",
      "143584/143613 [============================>.] - ETA: 0s - loss: 0.0423 - acc: 0.9836Epoch 00001: val_loss improved from 0.04607 to 0.04392, saving model to lstm_baseline.hdf5\n",
      "143613/143613 [==============================] - 1994s - loss: 0.0423 - acc: 0.9836 - val_loss: 0.0439 - val_acc: 0.9831\n",
      "153164/153164 [==============================] - 309s   \n"
     ]
    }
   ],
   "source": [
    "inp=Input(shape=(max_words_length,))\n",
    "x=Embedding(max_words_count,embedding_size,weights=[embedding_matrix])(inp)\n",
    "x=Bidirectional(LSTM(embedding_size,return_sequences=True,dropout=0.1,recurrent_dropout=0.1))(x)\n",
    "x=GlobalMaxPool1D()(x)\n",
    "x = Dense(50, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(6, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "file_path=\"lstm_baseline.hdf5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=20)\n",
    "callbacks_list = [checkpoint, early] #early\n",
    "model.fit(x_train, y, batch_size=32, epochs=2, validation_split=0.1, callbacks=callbacks_list)\n",
    "\n",
    "y_test = model.predict([x_test], batch_size=1024, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('./dataset/sample_submission.csv')\n",
    "sample_submission[classes] = y_test\n",
    "sample_submission.to_csv('submission_roc_auc/submission_baseline_lstm.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143584/143613 [============================>.] - ETA: 0s - loss: 0.0595 - acc: 0.9794Epoch 00000: val_loss improved from inf to 0.04869, saving model to lstm_preprocess2_best.hdf5\n",
      "143613/143613 [==============================] - 1717s - loss: 0.0595 - acc: 0.9794 - val_loss: 0.0487 - val_acc: 0.9822\n",
      "Epoch 2/2\n",
      "143584/143613 [============================>.] - ETA: 0s - loss: 0.0446 - acc: 0.9829Epoch 00001: val_loss improved from 0.04869 to 0.04683, saving model to lstm_preprocess2_best.hdf5\n",
      "143613/143613 [==============================] - 1720s - loss: 0.0446 - acc: 0.9829 - val_loss: 0.0468 - val_acc: 0.9823\n",
      "153164/153164 [==============================] - 242s   \n"
     ]
    }
   ],
   "source": [
    "inp=Input(shape=(max_words_length,))\n",
    "x=Embedding(max_words_count,embedding_size,weights=[embedding_matrix])(inp)\n",
    "x=Bidirectional(LSTM(embedding_size,return_sequences=True,dropout=0.1,recurrent_dropout=0.1))(x)\n",
    "x=GlobalMaxPool1D()(x)\n",
    "x = Dense(50, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(6, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "file_path=\"lstm_preprocess2_best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=20)\n",
    "callbacks_list = [checkpoint, early] #early\n",
    "model.fit(x_train, y, batch_size=32, epochs=2, validation_split=0.1, callbacks=callbacks_list)\n",
    "\n",
    "y_test = model.predict([x_test], batch_size=1024, verbose=1)\n",
    "sample_submission = pd.read_csv('./dataset/sample_submission.csv')\n",
    "sample_submission[classes] = y_test\n",
    "sample_submission.to_csv('submission/submission_preprocess2_lstm.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143584/143613 [============================>.] - ETA: 0s - loss: 0.0578 - acc: 0.9797Epoch 00000: val_loss improved from inf to 0.04744, saving model to gru_glovetwitter_best.hdf5\n",
      "143613/143613 [==============================] - 1365s - loss: 0.0578 - acc: 0.9797 - val_loss: 0.0474 - val_acc: 0.9827\n",
      "Epoch 2/2\n",
      "143584/143613 [============================>.] - ETA: 0s - loss: 0.0440 - acc: 0.9832Epoch 00001: val_loss improved from 0.04744 to 0.04588, saving model to gru_glovetwitter_best.hdf5\n",
      "143613/143613 [==============================] - 1363s - loss: 0.0440 - acc: 0.9832 - val_loss: 0.0459 - val_acc: 0.9825\n",
      "153164/153164 [==============================] - 188s   \n"
     ]
    }
   ],
   "source": [
    "from keras.layers import GRU\n",
    "inp_1=Input(shape=(max_words_length,))\n",
    "x_1=Embedding(max_words_count,embedding_size,weights=[embedding_matrix])(inp_1)\n",
    "x_1=Bidirectional(GRU(embedding_size,return_sequences=True,dropout=0.1,recurrent_dropout=0.1))(x_1)\n",
    "x_1=GlobalMaxPool1D()(x_1)\n",
    "x_1 = Dense(50, activation=\"relu\")(x_1)\n",
    "x_1 = Dropout(0.1)(x_1)\n",
    "x_1 = Dense(6, activation=\"sigmoid\")(x_1)\n",
    "model1 = Model(inputs=inp_1, outputs=x_1)\n",
    "model1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "file_path=\"gru_glovetwitter_best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=20)\n",
    "callbacks_list = [checkpoint, early] #early\n",
    "model1.fit(x_train, y, batch_size=32, epochs=2, validation_split=0.1, callbacks=callbacks_list)\n",
    "\n",
    "y_test_1 = model1.predict([x_test], batch_size=1024, verbose=1)\n",
    "sample_submission = pd.read_csv('./dataset/sample_submission.csv')\n",
    "sample_submission[classes] = y_test_1\n",
    "sample_submission.to_csv('submission/submission_glovetwitter_GRU_preprocess3_leaky.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_lstm='submission/submission_preprocess_lstm.csv'\n",
    "file_GRU='submission/submission_preprocess_GRU.csv'\n",
    "p_lstm = pd.read_csv(file_lstm)\n",
    "p_gru = pd.read_csv(file_GRU)\n",
    "\n",
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "p_res_avg = p_lstm.copy()\n",
    "p_res_avg[label_cols] = (p_gru[label_cols] + p_lstm[label_cols]) / 2\n",
    "\n",
    "p_res_avg.to_csv('submission_preprocess2_lstm+gru_avg.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_best='submission/submission_glovetwitter_GRU_0.0457.csv'\n",
    "data=pd.read_csv(file_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
