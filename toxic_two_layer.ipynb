{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_file='./dataset/glove.twitter.27B.50d.txt'\n",
    "train_file='./dataset/train.csv'\n",
    "test_file='./dataset/test.csv'\n",
    "\n",
    "train=pd.read_csv(train_file)\n",
    "test=pd.read_csv(test_file)\n",
    "\n",
    "sent_train=train[\"comment_text\"].fillna(\"nan\")\n",
    "\n",
    "classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y=train[classes].values\n",
    "\n",
    "sent_test=test[\"comment_text\"].fillna(\"nan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words_count=30000\n",
    "embedding_size=50\n",
    "max_words_length=150\n",
    "\n",
    "tokenizer=Tokenizer(num_words=max_words_count)\n",
    "tokenizer.fit_on_texts(sent_train)\n",
    "tokens_train = tokenizer.texts_to_sequences(sent_train)\n",
    "tokens_test = tokenizer.texts_to_sequences(sent_test)\n",
    "\n",
    "x_train=pad_sequences(tokens_train,maxlen=max_words_length)\n",
    "x_test=pad_sequences(tokens_test,maxlen=max_words_length)\n",
    "\n",
    "def index_to_embed(word,*embedding):\n",
    "    return word,np.asarray(embedding,dtype='float32')\n",
    "\n",
    "embed_dict=dict(index_to_embed(*o.strip().split())for o in open(glove_file))\n",
    "\n",
    "all_embs = np.stack(embed_dict.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "\n",
    "word_idx=tokenizer.word_index\n",
    "\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (max_words_count, embedding_size))\n",
    "\n",
    "for word,i in word_idx.items():\n",
    "    if i < max_words_count:\n",
    "        vec_temp=embed_dict.get(word)\n",
    "        if vec_temp is not None:\n",
    "            embedding_matrix[i]=vec_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143584/143613 [============================>.] - ETA: 0s - loss: 0.0554 - acc: 0.9804Epoch 00000: val_loss improved from inf to 0.04532, saving model to gru_wordcount30000_best.hdf5\n",
      "143613/143613 [==============================] - 1931s - loss: 0.0554 - acc: 0.9804 - val_loss: 0.0453 - val_acc: 0.9831\n",
      "Epoch 2/2\n",
      "143584/143613 [============================>.] - ETA: 0s - loss: 0.0422 - acc: 0.9837Epoch 00001: val_loss improved from 0.04532 to 0.04378, saving model to gru_wordcount30000_best.hdf5\n",
      "143613/143613 [==============================] - 1899s - loss: 0.0422 - acc: 0.9837 - val_loss: 0.0438 - val_acc: 0.9827\n",
      "153164/153164 [==============================] - 290s   \n"
     ]
    }
   ],
   "source": [
    "from keras.layers import GRU\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "inp_1=Input(shape=(max_words_length,))\n",
    "x_1=Embedding(max_words_count,embedding_size,weights=[embedding_matrix])(inp_1)\n",
    "x_1=Bidirectional(GRU(embedding_size,return_sequences=True,dropout=0.1,recurrent_dropout=0.1))(x_1)\n",
    "x_1=GlobalMaxPool1D()(x_1)\n",
    "x_1 = Dense(50, activation=\"relu\")(x_1)\n",
    "x_1 = Dropout(0.1)(x_1)\n",
    "x_1 = Dense(6, activation=\"sigmoid\")(x_1)\n",
    "model1 = Model(inputs=inp_1, outputs=x_1)\n",
    "model1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "file_path=\"gru_wordcount30000_best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=20)\n",
    "callbacks_list = [checkpoint, early] #early\n",
    "model1.fit(x_train, y, batch_size=32, epochs=2, validation_split=0.1, callbacks=callbacks_list)\n",
    "\n",
    "y_test_1 = model1.predict([x_test], batch_size=1024, verbose=1)\n",
    "sample_submission = pd.read_csv('./dataset/sample_submission.csv')\n",
    "sample_submission[classes] = y_test_1\n",
    "sample_submission.to_csv('submission/submission_wordcount30000baseline_GRU_.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "file_best='submission/submission_wordcount30000baseline_GRU_.csv'\n",
    "data=pd.read_csv(file_best)\n",
    "data[classes]=scipy.special.expit(scipy.special.logit(data[classes])-0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "sample_submission1 = pd.read_csv('./dataset/sample_submission.csv')\n",
    "sample_submission2= pd.read_csv('./dataset/sample_submission.csv')\n",
    "sample_submission1[label_cols] = data[label_cols]\n",
    "sample_submission2[label_cols]=data[label_cols]/1.4\n",
    "sample_submission1.to_csv('submission/submission_batchnorm_postprocessing_expit.csv',index=False)\n",
    "sample_submission2.to_csv('submission/submission_batchnorm_postprocessing_1.4.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission5 = pd.read_csv('submission_glovetwitter_GRU.csv')\n",
    "sample_submission5[label_cols]=sample_submission3[label_cols]/1.4\n",
    "sample_submission5.to_csv('submission/submission_glovetwitter_postprocessing_1.4v2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
